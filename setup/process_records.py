import pandas as pd
import numpy as np
import re
from datetime import datetime


def parse_complex_classes(class_str):
    """
    Parse les classes complexes et retourne une liste de classes individuelles

    Exemples:
    - "T46/47/48" -> ["T46", "T47", "T48"]
    - "T46-48" -> ["T46", "T47", "T48"]
    - "T42-47/61-64" -> ["T42", "T43", "T44", "T45", "T46", "T47", "T61", "T62", "T63", "T64"]
    - "F11" -> ["F11"]
    """

    if pd.isna(class_str) or class_str == '':
        return []

    class_str = str(class_str).strip()
    all_classes = []

    # Diviser par "/" pour traiter chaque groupe sÃ©parÃ©ment
    groups = class_str.split('/')

    for group in groups:
        group = group.strip()

        # VÃ©rifier s'il y a un tiret (range)
        if '-' in group:
            # Extraire la lettre prÃ©fixe et les numÃ©ros
            match = re.match(r'([A-Z]+)(\d+)-(\d+)', group)
            if match:
                prefix = match.group(1)  # T, F, P, etc.
                start_num = int(match.group(2))
                end_num = int(match.group(3))

                # GÃ©nÃ©rer toutes les classes dans la plage
                for num in range(start_num, end_num + 1):
                    all_classes.append(f"{prefix}{num}")
            else:
                # Format non reconnu, ajouter tel quel
                all_classes.append(group)
        else:
            # Pas de tiret, classe simple
            all_classes.append(group)

    # Supprimer les doublons et trier
    unique_classes = sorted(list(set(all_classes)))
    return unique_classes


def parse_date_safe(date_value):
    """
    Parse une date de maniÃ¨re sÃ©curisÃ©e

    Returns:
        datetime ou None si parsing impossible
    """
    if pd.isna(date_value):
        return None

    try:
        # Si c'est dÃ©jÃ  un datetime
        if isinstance(date_value, datetime):
            return date_value

        # Si c'est un timestamp pandas
        if hasattr(date_value, 'to_pydatetime'):
            return date_value.to_pydatetime()

        # Essayer de parser comme string
        return pd.to_datetime(str(date_value))
    except:
        return None


def remove_duplicates_keep_latest(df):
    """
    Supprime les doublons en gardant le plus rÃ©cent basÃ© sur record_date

    CritÃ¨res de doublon : gender, event, class, record_type, region_code

    Returns:
        tuple: (df_cleaned, removed_count, duplicate_info)
    """

    # Colonnes Ã  vÃ©rifier pour les doublons
    duplicate_check_columns = ['gender', 'event', 'class', 'record_type', 'region_code']

    # VÃ©rifier que toutes les colonnes existent
    missing_cols = [col for col in duplicate_check_columns if col not in df.columns]
    if missing_cols:
        print(f"âš ï¸ Colonnes manquantes pour la vÃ©rification des doublons : {missing_cols}")
        return df, 0, []

    if 'record_date' not in df.columns:
        print(f"âš ï¸ Colonne 'record_date' manquante - impossible de dÃ©terminer le plus rÃ©cent")
        return df, 0, []

    # CrÃ©er une copie pour le traitement
    df_work = df.copy()

    # Parser les dates de maniÃ¨re sÃ©curisÃ©e
    df_work['parsed_date'] = df_work['record_date'].apply(parse_date_safe)

    # Identifier les doublons
    duplicates_mask = df_work.duplicated(subset=duplicate_check_columns, keep=False)

    if not duplicates_mask.any():
        print(f"âœ… Aucun doublon dÃ©tectÃ© !")
        return df, 0, []

    duplicates = df_work[duplicates_mask].copy()

    print(f"\nğŸ” DOUBLONS DÃ‰TECTÃ‰S - TRAITEMENT EN COURS...")
    print(f"=" * 60)
    print(f"ğŸ“Š Nombre de lignes avec doublons : {len(duplicates)}")

    # Grouper les doublons
    duplicate_groups = duplicates.groupby(duplicate_check_columns)

    removed_info = []
    indices_to_remove = []

    print(f"\nğŸ“‹ ANALYSE DES GROUPES DE DOUBLONS :")
    print(f"-" * 60)

    for group_key, group_df in duplicate_groups:
        gender, event, class_name, record_type, region_code = group_key

        print(f"\nğŸ”¸ Groupe : {gender} | {event} | {class_name} | {record_type} | {region_code}")
        print(f"   Nombre de doublons : {len(group_df)}")

        # Trier par date (les plus rÃ©centes en premier, NaT/None Ã  la fin)
        group_sorted = group_df.sort_values(
            'parsed_date',
            ascending=False,
            na_position='last'
        )

        # Garder le premier (plus rÃ©cent)
        keep_index = group_sorted.index[0]
        remove_indices = group_sorted.index[1:].tolist()

        # Informations sur ce qui est gardÃ© vs supprimÃ©
        kept_row = group_sorted.iloc[0]
        removed_rows = group_sorted.iloc[1:]

        print(
            f"   ğŸŸ¢ GARDÃ‰   : SDMS {kept_row['sdms']} | {kept_row['npc']} | Date: {kept_row['record_date']} | Perf: {kept_row['performance']}")

        for _, removed_row in removed_rows.iterrows():
            print(
                f"   ğŸ”´ SUPPRIMÃ‰: SDMS {removed_row['sdms']} | {removed_row['npc']} | Date: {removed_row['record_date']} | Perf: {removed_row['performance']}")

            removed_info.append({
                'group': group_key,
                'removed_sdms': removed_row['sdms'],
                'removed_npc': removed_row['npc'],
                'removed_date': removed_row['record_date'],
                'removed_performance': removed_row['performance'],
                'kept_sdms': kept_row['sdms'],
                'kept_npc': kept_row['npc'],
                'kept_date': kept_row['record_date'],
                'kept_performance': kept_row['performance']
            })

        indices_to_remove.extend(remove_indices)

    # Supprimer les lignes dupliquÃ©es (garder seulement les plus rÃ©centes)
    df_cleaned = df_work.drop(indices_to_remove).copy()

    # Supprimer la colonne temporaire
    df_cleaned = df_cleaned.drop('parsed_date', axis=1)

    removed_count = len(indices_to_remove)

    print(f"\nğŸ“ˆ RÃ‰SUMÃ‰ DU NETTOYAGE :")
    print(f"   â€¢ Lignes avant nettoyage : {len(df)}")
    print(f"   â€¢ Lignes supprimÃ©es : {removed_count}")
    print(f"   â€¢ Lignes aprÃ¨s nettoyage : {len(df_cleaned)}")

    return df_cleaned, removed_count, removed_info


def process_records_excel(excel_file_path, sheet_name=0):
    """
    Traite un fichier Excel de records et expanse les classes complexes

    Args:
        excel_file_path: Chemin vers le fichier Excel
        sheet_name: Nom ou index de la feuille (dÃ©faut: premiÃ¨re feuille)

    Returns:
        DataFrame avec les lignes dupliquÃ©es pour chaque classe et doublons supprimÃ©s
    """

    try:
        # Lecture du fichier Excel
        print(f"ğŸ“– Lecture du fichier Excel : {excel_file_path}")
        df = pd.read_excel(excel_file_path, sheet_name=sheet_name)

        print(f"ğŸ“Š DonnÃ©es chargÃ©es : {len(df)} lignes, {len(df.columns)} colonnes")
        print(f"ğŸ“‹ Colonnes disponibles : {list(df.columns)}")

    except FileNotFoundError:
        print(f"âŒ Erreur : Fichier non trouvÃ© - {excel_file_path}")
        return None
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture du fichier : {e}")
        return None

    # Nettoyage des noms de colonnes
    df.columns = df.columns.str.strip()

    # VÃ©rification des colonnes requises (sans firstname et lastname)
    required_columns = ['gender', 'event', 'class', 'sdms', 'npc', 'performance',
                        'record_type', 'region_code', 'record_date', 'City', 'Country']

    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"âŒ Colonnes manquantes : {missing_columns}")
        return None

    # Filtrer les lignes avec SDMS rempli
    initial_rows = len(df)
    df = df[df['sdms'].notna() & (df['sdms'] != '')].copy()
    print(f"ğŸ” Lignes avec SDMS rempli : {len(df)}/{initial_rows}")

    if len(df) == 0:
        print("âš ï¸ Aucune ligne avec SDMS trouvÃ©e")
        return pd.DataFrame()

    # CrÃ©ation de la colonne location
    df['location'] = df['City'].astype(str) + ", " + df['Country'].astype(str)
    # Nettoyer les cas oÃ¹ City ou Country sont vides
    df['location'] = df['location'].str.replace('nan, ', '').str.replace(', nan', '').str.replace('nan, nan', '')

    print(f"ğŸƒ Expansion des classes complexes...")

    # Liste pour stocker les lignes expansÃ©es
    expanded_rows = []

    # Compteurs pour les statistiques
    complex_classes_count = 0
    total_expanded_rows = 0

    for index, row in df.iterrows():
        class_value = row['class']

        # Parser les classes
        individual_classes = parse_complex_classes(class_value)

        if len(individual_classes) > 1:
            complex_classes_count += 1
            print(f"   ğŸ“ '{class_value}' -> {individual_classes}")

        # Si aucune classe trouvÃ©e, garder la ligne originale
        if not individual_classes:
            individual_classes = [class_value]

        # CrÃ©er une ligne pour chaque classe individuelle
        for individual_class in individual_classes:
            new_row = row.copy()
            new_row['class'] = individual_class
            expanded_rows.append(new_row)
            total_expanded_rows += 1

    # CrÃ©er le DataFrame final
    result_df = pd.DataFrame(expanded_rows)

    # SÃ©lectionner et rÃ©organiser les colonnes finales (SANS firstname et lastname)
    final_columns = [
        'gender', 'event', 'class', 'sdms', 'npc', 'performance',
        'record_type', 'region_code', 'record_date', 'location'
    ]

    result_df = result_df[final_columns].reset_index(drop=True)

    print(f"\nğŸ“ˆ STATISTIQUES D'EXPANSION :")
    print(f"   â€¢ Lignes originales avec SDMS : {len(df)}")
    print(f"   â€¢ Classes complexes trouvÃ©es : {complex_classes_count}")
    print(f"   â€¢ Lignes aprÃ¨s expansion : {len(result_df)}")
    print(f"   â€¢ Lignes ajoutÃ©es par expansion : {len(result_df) - len(df)}")

    # ğŸš¨ SUPPRESSION DES DOUBLONS (GARDER LE PLUS RÃ‰CENT)
    print(f"\nğŸ”§ NETTOYAGE DES DOUBLONS...")
    cleaned_df, removed_count, removed_info = remove_duplicates_keep_latest(result_df)

    if removed_count > 0:
        print(f"\nâœ… Nettoyage terminÃ© : {removed_count} doublons supprimÃ©s")

    return cleaned_df


def analyze_expanded_data(df):
    """Analyse les donnÃ©es expansÃ©es et nettoyÃ©es"""

    if df is None or len(df) == 0:
        return

    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š ANALYSE DES DONNÃ‰ES FINALES")
    print(f"=" * 60)

    # Statistiques gÃ©nÃ©rales
    print(f"ğŸ“ˆ Total des enregistrements : {len(df)}")
    print(f"ğŸƒ AthlÃ¨tes uniques (SDMS) : {df['sdms'].nunique()}")
    print(f"ğŸ† Ã‰vÃ©nements uniques : {df['event'].nunique()}")
    print(f"ğŸ¯ Classes uniques : {df['class'].nunique()}")
    print(f"ğŸŒ Pays uniques : {df['npc'].nunique()}")

    # Top Ã©vÃ©nements
    print(f"\nğŸƒ Top 10 Ã©vÃ©nements :")
    event_counts = df['event'].value_counts().head(10)
    for event, count in event_counts.items():
        print(f"   â€¢ {event} : {count}")

    # Top classes
    print(f"\nğŸ¯ Top 10 classes :")
    class_counts = df['class'].value_counts().head(10)
    for class_name, count in class_counts.items():
        print(f"   â€¢ {class_name} : {count}")

    # Top pays
    print(f"\nğŸŒ Top 10 pays :")
    npc_counts = df['npc'].value_counts().head(10)
    for npc, count in npc_counts.items():
        print(f"   â€¢ {npc} : {count}")

    # Types de records
    if 'record_type' in df.columns:
        print(f"\nğŸ† Types de records :")
        record_counts = df['record_type'].value_counts()
        for record_type, count in record_counts.items():
            print(f"   â€¢ {record_type} : {count}")

    # RÃ©partition par genre
    if 'gender' in df.columns:
        print(f"\nğŸ‘¥ RÃ©partition par genre :")
        gender_counts = df['gender'].value_counts()
        for gender, count in gender_counts.items():
            print(f"   â€¢ {gender} : {count}")


def save_expanded_data(df, output_filename='expanded_records_cleaned.xlsx'):
    """Sauvegarder les donnÃ©es expansÃ©es et nettoyÃ©es"""

    if df is None or len(df) == 0:
        print("âŒ Aucune donnÃ©e Ã  sauvegarder")
        return False

    try:
        df.to_excel(output_filename, index=False)
        print(f"\nâœ… Fichier sauvegardÃ© avec succÃ¨s : {output_filename}")
        return True
    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde : {e}")
        return False


def test_class_parsing():
    """Teste la fonction de parsing des classes"""

    print(f"\nğŸ§ª TEST DE PARSING DES CLASSES:")
    print(f"=" * 40)

    test_cases = [
        "T46/47/48",
        "T46-48",
        "T42-47/61-64",
        "F11",
        "T20/F20",
        "P44-47",
        "T11-13/51-54",
        ""
    ]

    for test_case in test_cases:
        result = parse_complex_classes(test_case)
        print(f"'{test_case}' -> {result}")


# UTILISATION DU SCRIPT
if __name__ == "__main__":

    # Test de parsing (optionnel)
    test_class_parsing()

    # âš ï¸  MODIFIEZ CE CHEMIN VERS VOTRE FICHIER EXCEL âš ï¸
    excel_file_path = "C:\\Users\khalil.mzoughi\PycharmProjects\Tunis-Athletics-GP\setup\\records.xlsx"  # ğŸ“ Remplacez par le chemin de votre fichier

    # Optionnel : spÃ©cifier le nom de la feuille si nÃ©cessaire
    sheet_name = 0  # 0 = premiÃ¨re feuille, ou utilisez le nom : "Sheet1"

    print(f"\nğŸš€ DÃ‰MARRAGE DU TRAITEMENT DES RECORDS...")

    # Traitement du fichier Excel
    cleaned_df = process_records_excel(excel_file_path, sheet_name=sheet_name)

    # VÃ©rification du succÃ¨s du traitement
    if cleaned_df is not None and len(cleaned_df) > 0:

        # Analyse des rÃ©sultats
        analyze_expanded_data(cleaned_df)

        # AperÃ§u des donnÃ©es
        print(f"\nğŸ‘€ APERÃ‡U DES DONNÃ‰ES FINALES (5 premiÃ¨res lignes):")
        print(cleaned_df.head().to_string())

        # VÃ©rification finale : plus de doublons
        duplicate_check_columns = ['gender', 'event', 'class', 'record_type', 'region_code']
        final_duplicates = cleaned_df[cleaned_df.duplicated(subset=duplicate_check_columns, keep=False)]

        if len(final_duplicates) == 0:
            print(f"\nâœ… VÃ‰RIFICATION FINALE : Aucun doublon restant !")
        else:
            print(f"\nâš ï¸ ATTENTION : {len(final_duplicates)} doublons restants dÃ©tectÃ©s")

        # Sauvegarde des rÃ©sultats
        output_file = "expanded_records_cleaned.xlsx"
        success = save_expanded_data(cleaned_df, output_file)

        if success:
            print(f"\nğŸ‰ Traitement terminÃ© avec succÃ¨s !")
            print(f"ğŸ“‚ Fichier de sortie : {output_file}")
            print(f"ğŸ“Š DonnÃ©es finales : {len(cleaned_df)} enregistrements uniques")

    else:
        print("âŒ Ã‰chec du traitement - VÃ©rifiez votre fichier Excel")